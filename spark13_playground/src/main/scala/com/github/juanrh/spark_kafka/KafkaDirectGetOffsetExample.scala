package com.github.juanrh.spark_kafka

import org.apache.spark.SparkConf
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.kafka.HasOffsetRanges
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.kafka.OffsetRange

import kafka.serializer.StringDecoder

/** Simple Spark Streaming job that reads from Kafka with DirectKafkaInputDStream and gets the Kafka offset for 
 *  the micro batches: a start and end offset is assigned to each partition of the micro batch, due to the 1 to 1 
 *  correspondence between Kafka and RDD partitions in  DirectKafkaInputDStream. See https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md
 *  Expects String in the key and value, and so can be used writing with the test Kafka producer console.
 *  This program makes clear offsets in Kafka are just counters, no timestamps corresponds to some epoch or anything. I
 *  guess this is due to Kafka being an asynchronous distributed system. Anayway offsets still establish a total order
 *  per topic partitions, and that is very useful   
 * */
object KafkaDirectGetOffsetExample extends App {   
  // Configuration
  val topic = "test"
  val seedKafkaBroker = "localhost" 

  // Create Spark Streaming context
  val master = "local[3]"
  val batchDuration = Seconds(1)
  val conf = new SparkConf().setMaster(master).setAppName("KafkaDirectExample")
  val ssc : StreamingContext = new StreamingContext(conf, batchDuration)
  
  // Connect to a Kafka topic for reading
  val kafkaParams : Map[String, String] = Map("metadata.broker.list" -> (seedKafkaBroker + ":9092"))
  val kafkaInputStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set(topic))  
  
  /* Get the offsets using HasOffsetRanges (see https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md).
   * Note that way we do not get a single offset for each message, but a range, because kafka messages with different 
   * offsets have been put together in a single RDD for the micro batch, and the offset information in HasOffsetRanges comes
   * at the level of RDD. The other approach based on the advanced use of KafkaUtils.createDirectStream would obtain a different 
   * offset per message, more precise but more difficult
   * */		
  val kafkaStreamWithOffsets = kafkaInputStream.transform((rdd, _time) => {
    // Cast the rdd to an interface that lets us get a collection of offset ranges
    // I guess offsets(i) is the offset for the Kafka partition i 
    val offsets : Array[OffsetRange]= rdd.asInstanceOf[HasOffsetRanges].offsetRanges
    rdd.mapPartitionsWithIndex { (rddPartitionIndex, iter) =>
       /*
       Use rddPartitionIndex to get the correct offset range for the rdd partition we're working on
       In https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md says "The important thing to 
       notice here is there is a 1:1 correspondence between Kafka partition and RDD partition". Hence i corresponds
       to a Kafka partition only because we haven't used repartition on kafkaInputStream. So if we want to get 
       the offsets like this then that should be the first operation on the DStream. Probably the cast rdd.asInstanceOf[HasOffsetRanges] 
       would fail is applied to rdd.repartition(...), or inside a transform on kafkaInputStream.repartition(...)
       Note even there is a 1:1 correspondence between RDD and Kafka partitions, the mapping between them might be
       unknown: e.g. Kafka partition 1 could go to RDD partition 3. Besides in a the KafkaRDD generated by a Kafka
       batch load to Spark, be could even be loading from several topics at the same time, getting more RDD partitions
       that Kafka partitions for one of the topics. 
       */
      val offsetRange: OffsetRange = offsets(rddPartitionIndex)
      iter.map { case (k, v) =>  KeyValOffset(k, v, offsetRange.partition, offsetRange.fromOffset, offsetRange.untilOffset) }
    }
   }
  )
   
  // check the connection  
  // kafkaInputStream.print
  kafkaStreamWithOffsets.print
  
  ssc.start
  ssc.awaitTermination
  
  // TODO: check Avro events generator from Confluent
}